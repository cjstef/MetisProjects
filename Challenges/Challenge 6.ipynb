{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Generate (fake) data that is linearly related to log(x).\n",
    "\n",
    "You are making this model up. It is of the form B0 + B1*log(x) + epsilon. (You are making up the parameters.)\n",
    "\n",
    "Simulate some data from this model.\n",
    "\n",
    "Then fit two models to it:\n",
    "\n",
    "quadratic (second degree polynomial)\n",
    "logarithmic (log(x))\n",
    "(The second one should fit really well, since it has the same form as the underlying model!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.008366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.324542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.861071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.872222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.627128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           y\n",
       "0  15.008366\n",
       "1  18.324542\n",
       "2  19.861071\n",
       "3  20.872222\n",
       "4  21.627128"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "d = np.random.randint(1,10)*np.log(np.linspace(1,100))+np.random.randint(10,50)+np.random.normal()\n",
    "\n",
    "#fake data! Sad! pathetic!\n",
    "\n",
    "df = pd.DataFrame(d, columns=['y'])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.589</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.581</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   68.83</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 07 Feb 2017</td> <th>  Prob (F-statistic):</th> <td>7.88e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:34:30</td>     <th>  Log-Likelihood:    </th> <td> -101.87</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   207.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    48</td>      <th>  BIC:               </th> <td>   211.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>int</th> <td>   23.3601</td> <td>    0.404</td> <td>   57.767</td> <td> 0.000</td> <td>   22.547    24.173</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X1</th>  <td>    0.0029</td> <td>    0.000</td> <td>    8.296</td> <td> 0.000</td> <td>    0.002     0.004</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>42.511</td> <th>  Durbin-Watson:     </th> <td>   0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 142.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-2.323</td> <th>  Prob(JB):          </th> <td>1.33e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 9.831</td> <th>  Cond. No.          </th> <td>1.73e+03</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.589\n",
       "Model:                            OLS   Adj. R-squared:                  0.581\n",
       "Method:                 Least Squares   F-statistic:                     68.83\n",
       "Date:                Tue, 07 Feb 2017   Prob (F-statistic):           7.88e-11\n",
       "Time:                        15:34:30   Log-Likelihood:                -101.87\n",
       "No. Observations:                  50   AIC:                             207.7\n",
       "Df Residuals:                      48   BIC:                             211.6\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "int           23.3601      0.404     57.767      0.000        22.547    24.173\n",
       "X1             0.0029      0.000      8.296      0.000         0.002     0.004\n",
       "==============================================================================\n",
       "Omnibus:                       42.511   Durbin-Watson:                   0.095\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              142.188\n",
       "Skew:                          -2.323   Prob(JB):                     1.33e-31\n",
       "Kurtosis:                       9.831   Cond. No.                     1.73e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.73e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['int'] = 1\n",
    "df['X1'] = [x for x in range(1,len(d)+1)]\n",
    "x = df.drop('y',axis=1)\n",
    "y = df.y\n",
    "\n",
    "#quadratic\n",
    "\n",
    "x2 = x**2\n",
    "\n",
    "lsm = sm.OLS(y,x2)\n",
    "fit = lsm.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.969</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1529.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 07 Feb 2017</td> <th>  Prob (F-statistic):</th> <td>1.31e-38</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:35:15</td>     <th>  Log-Likelihood:    </th> <td> -147.11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   296.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    49</td>      <th>  BIC:               </th> <td>   298.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>int</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0         0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X1</th>  <td>    8.2742</td> <td>    0.212</td> <td>   39.104</td> <td> 0.000</td> <td>    7.849     8.699</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>13.661</td> <th>  Durbin-Watson:     </th> <td>   0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  14.509</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.228</td> <th>  Prob(JB):          </th> <td>0.000707</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.964</td> <th>  Cond. No.          </th> <td>     inf</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.969\n",
       "Model:                            OLS   Adj. R-squared:                  0.968\n",
       "Method:                 Least Squares   F-statistic:                     1529.\n",
       "Date:                Tue, 07 Feb 2017   Prob (F-statistic):           1.31e-38\n",
       "Time:                        15:35:15   Log-Likelihood:                -147.11\n",
       "No. Observations:                  50   AIC:                             296.2\n",
       "Df Residuals:                      49   BIC:                             298.1\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "int                 0          0        nan        nan             0         0\n",
       "X1             8.2742      0.212     39.104      0.000         7.849     8.699\n",
       "==============================================================================\n",
       "Omnibus:                       13.661   Durbin-Watson:                   0.016\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               14.509\n",
       "Skew:                           1.228   Prob(JB):                     0.000707\n",
       "Kurtosis:                       3.964   Cond. No.                          inf\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is      0. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logarithmic\n",
    "xl = np.log(x)\n",
    "\n",
    "lsm = sm.OLS(y,xl)\n",
    "fit = lsm.fit()\n",
    "fit.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "Generate (fake) data from a model of the form B0 + B1*x + B2*x^2 + epsilon. (You are making up the parameters.)\n",
    "\n",
    "Split the data into a training and test set.\n",
    "\n",
    "Fit a model to your training set. Calculate mean squared error on your training set. Then calculate it on your test set.\n",
    "\n",
    "(You could use sklearn.metrics.mean_squared_error.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d2 = np.random.randint(1,10)*np.linspace(1,100)**2+np.random.randint(10,50)+np.random.normal()+np.random.randint(1,10)*np.linspace(1,100)\n",
    "\n",
    "df2 = pd.DataFrame(d2, columns=['y'])\n",
    "\n",
    "df2['X1'] = [x for x in range(1,len(d)+1)]\n",
    "x = df2.drop('y',axis=1)\n",
    "y = df2.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766376726.801 792921109.758\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "% matplotlib inline\n",
    "\n",
    "lr = LinearRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)\n",
    "mse_etrain = mean_squared_error(X_train, y_train)\n",
    "mse_etest = mean_squared_error(X_test, y_test)\n",
    "\n",
    "print mse_etrain, mse_etest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "For the data from two (above), try polynomial fits from 0th (just constant) to 7th order (highest term x^7). Over the x axis of model degree (8 points), plot:\n",
    "\n",
    "training error\n",
    "test error\n",
    "R squared\n",
    "AIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "For the data from two (above), fit a model to only the first 5 of your data points (m=5). Then to first 10 (m=10). Then to first 15 (m=15). In this manner, keep fitting until you fit your entire training set. For each step, calculate the training error and the test error. Plot both (in the same plot) over m. This is called a learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
